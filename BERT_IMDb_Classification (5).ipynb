{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"text-align: left; color: #4CAF50; font-size: 24px; font-weight: bold; font-family: Arial;\">\n",
        "Sentiment Analysis using BERT: Predicting Sentiment in IMDB Reviews\n",
        "</h1>\n",
        "\n",
        "<div style=\"text-align: left; font-size: 18px; color: #333; font-style: italic; margin-top: 10px; font-family: Arial;\">\n",
        "Author: Prashant Sundge\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "CqblYwF_acVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Index: Sentiment Analysis using BERT\n",
        "\n",
        "## 1. Loading Pre-Trained BERT Model\n",
        "   - Initialize and load a pre-trained BERT model for fine-tuning.\n",
        "\n",
        "## 2. Fine-Tuning BERT for Specific Task\n",
        "   - Prepare BERT for sequence classification on IMDB sentiment analysis.\n",
        "\n",
        "## 3. IMDB Dataset Loading\n",
        "   - Load the IMDB dataset consisting of movie reviews labeled with sentiment (positive or negative).\n",
        "\n",
        "## 4. Create Labels\n",
        "   - Prepare labels for sentiment classes based on the dataset.\n",
        "\n",
        "## 5. Data Splitting: 2K Training, 100 Testing\n",
        "   - Split the dataset into a training set with 2000 samples and a test set with 100 samples.\n",
        "\n",
        "## 6. Save the Model\n",
        "   - Save the fine-tuned BERT model after training.\n",
        "\n",
        "## 7. Load the Model\n",
        "   - Load the saved BERT model for inference.\n",
        "\n",
        "## 8. Testing the Model on New Data\n",
        "   - Tokenize new data using BERT's tokenizer.\n",
        "   - Convert inputs to PyTorch tensors.\n",
        "   - Make predictions with the loaded model.\n",
        "   - Extract predicted labels.\n",
        "\n",
        "## 9. Display Predictions\n",
        "   - Show predictions made by the model on new data.\n",
        "\n",
        "## 10. Model Evaluation and Metrics\n",
        "   - Generate a classification report showing precision, recall, and F1-score.\n",
        "   - Display a confusion matrix for further evaluation.\n",
        "\n",
        "## 11. Create DataFrame for Reviews, Actual, and Predicted Labels\n",
        "   - Organize results into a DataFrame for better visualization and analysis.\n",
        "\n",
        "## 12. The End\n",
        "   - Conclusion and summary of the project's key findings.\n"
      ],
      "metadata": {
        "id": "ZHnfyjbcdvEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-family: Arial, sans-serif; font-size: 18px; line-height: 1.6; text-align: justify; color: #333;\">\n",
        "<b>This project utilizes BERT, a state-of-the-art transformer model, for sentiment analysis on the IMDB dataset.</b> The IMDB dataset consists of movie reviews labeled as positive or negative sentiments. By fine-tuning BERT for sequence classification, the model predicts sentiment labels based on the textual content of reviews. The dataset is split into a training set of 2000 samples and a test set of 100 samples for evaluation. Performance metrics such as accuracy, precision, recall, and F1-score demonstrate the model's capability to accurately classify sentiment in natural language, showcasing the application of advanced NLP techniques in understanding and analyzing textual sentiment.\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "xqFfpHNWa1he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessory Libraries"
      ],
      "metadata": {
        "id": "9jmkQibmEp7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Install Transformers and Torch libraries: `!pip install transformers torch -q`\n",
        "- Install with Accelerate for enhanced performance: `!pip install accelerate -U`\n",
        "- Simplified install command for all: `!pip install transformers torch accelerate -q`\n"
      ],
      "metadata": {
        "id": "lOv248ztYE2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch -q"
      ],
      "metadata": {
        "id": "J73fMIVlEvpU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers[torch] -q"
      ],
      "metadata": {
        "id": "i7koWpdVVsSd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U\n",
        "\n",
        "# # Step 2: Install the required libraries\n",
        "# !pip install transformers torch accelerate -q"
      ],
      "metadata": {
        "id": "xAPg14ydVg0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec43a9f6-553a-42b2-b30d-77a04df1aabe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/314.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/314.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.32.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading PreTrained  BERT Model"
      ],
      "metadata": {
        "id": "fYplJuu4E3qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BERT Tokenizer and Model Usage**:\n",
        "  - Import `BertTokenizer` and `BertModel` from `transformers`.\n",
        "  - Load pre-trained tokenizer and model using `'bert-base-uncased'`.\n",
        "  - Tokenize input text with `tokenizer(text, return_tensors='pt')`.\n",
        "  - Obtain embeddings using `model(**inputs).last_hidden_state`.\n",
        "  - Print the shape of embeddings (`torch.Size([1, seq_length, 768]`) for BERT base model.\n"
      ],
      "metadata": {
        "id": "LpexMenPYTke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "# load pre-trained model and tokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# tokenizer input text\n",
        "text= \"Hello, how are you ?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors = 'pt')\n",
        "\n",
        "\n",
        "# get the embeddings\n",
        "outputs = model(**inputs)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(last_hidden_states.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx4o2gWhEzOR",
        "outputId": "f83c8607-2b4d-455e-ce27-1d020bfb5ef0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning BERT for Specific Task"
      ],
      "metadata": {
        "id": "4sbCHuqgN_ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer , TrainingArguments\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset"
      ],
      "metadata": {
        "id": "rmzo7OzxFHf5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMBD DATASET LOAD"
      ],
      "metadata": {
        "id": "JFB7A9-llwgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-65erBOQmZJQ",
        "outputId": "a24f627b-33c4-458e-ee6a-193b9f4fd7a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# i have changed the Colab as earlier i used zip file so the # code is for that but its manageable\n",
        "\n",
        "# zip_path =\"/content/drive/MyDrive/Colab Notebooks/archive/IMDB Dataset.csv\"\n",
        "extract_path = '/content/drive/MyDrive/Colab Notebooks/archive/IMDB Dataset.csv'\n",
        "\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# extracted_files = os.listdir(extract_path)\n",
        "# print(f\"Extracted files {extracted_files}\")\n",
        "\n",
        "# dfs = []\n",
        "\n",
        "# for file in extracted_files:\n",
        "#   if file.endswith('.csv'):\n",
        "#     file_path=os.path.join(extract_path, file)\n",
        "#     df= pd.read_csv(file_path)\n",
        "#     dfs.append(df)"
      ],
      "metadata": {
        "id": "K5v6RFTslzcq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head()\n",
        "df = pd.read_csv(extract_path)"
      ],
      "metadata": {
        "id": "VW71RlA5naFU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cADEBGUTyodX",
        "outputId": "4bde80ed-90fa-4fb4-b29a-c320a383101f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "positive    25000\n",
              "negative    25000\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Labels"
      ],
      "metadata": {
        "id": "OkTzPNDEYoPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiments_to_labels(sentiment):\n",
        "  if sentiment == \"positive\":\n",
        "      return 1\n",
        "  elif sentiment =='negative':\n",
        "      return 0\n",
        "  else:\n",
        "      return None\n",
        "\n",
        "df['labels'] = df['sentiment'].apply(sentiments_to_labels)\n",
        "\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LgdLuXswywpW",
        "outputId": "5b7528ad-4be7-43eb-cff5-957d7815e58e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment  labels\n",
              "0  One of the other reviewers has mentioned that ...  positive       1\n",
              "1  A wonderful little production. <br /><br />The...  positive       1\n",
              "2  I thought this was a wonderful way to spend ti...  positive       1\n",
              "3  Basically there's a family where a little boy ...  negative       0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3a1ab9d-e28d-4eed-b1b3-11cb3428b8aa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3a1ab9d-e28d-4eed-b1b3-11cb3428b8aa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3a1ab9d-e28d-4eed-b1b3-11cb3428b8aa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3a1ab9d-e28d-4eed-b1b3-11cb3428b8aa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-25fd9ccb-4fc2-43d8-96c6-7fae57948460\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25fd9ccb-4fc2-43d8-96c6-7fae57948460')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-25fd9ccb-4fc2-43d8-96c6-7fae57948460 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data and use only 2k for training and 100 for testing"
      ],
      "metadata": {
        "id": "eVOzE5sCGr_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns='sentiment')"
      ],
      "metadata": {
        "id": "EhZOV8EeHwyO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "\n",
        "texts = df['review'].head(2000)\n",
        "labels= df['labels'].head(2000)\n",
        "\n",
        "\n",
        "# testing datase created 100 for evaluation\n",
        "test_reviews = list(df['review'].head(100))\n",
        "test_labels =  list(df['labels'].head(100))\n",
        "\n"
      ],
      "metadata": {
        "id": "8XC2vXaLG0Z_"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Preparation\n",
        "\n",
        "- **Dataset Class**: Defined `sampleDataset` class inherits from `Dataset`, designed to handle text classification tasks using BERT.\n",
        "  - **Initialization**: Accepts `texts`, `labels`, `tokenizer`, and `max_len` parameters.\n",
        "  - **__len__ Method**: Returns the length of the dataset.\n",
        "  - **__getitem__ Method**: Tokenizes each text using the tokenizer, truncates/pads to `max_len`, and returns `input_ids`, `attention_mask`, and `labels`.\n",
        "\n",
        "- **Data Preparation**:\n",
        "  - `texts` and `labels` are extracted from `df['review']` and `df['labels']` respectively.\n",
        "  - `tokenizer` is initialized using `BertTokenizer.from_pretrained`.\n",
        "  - `dataset` is created using `sampleDataset` with `texts`, `labels`, `tokenizer`, and `max_len=32`.\n",
        "  - `dataloader` is created using `DataLoader` for batching.\n",
        "\n",
        "### Model Training\n",
        "\n",
        "- **Model Loading**:\n",
        "  - `model` is loaded using `BertForSequenceClassification.from_pretrained` with `num_labels=2` for binary classification.\n",
        "\n",
        "- **Training Arguments**:\n",
        "  - Initial training arguments (commented out) include output directory, epochs, batch size, and logging settings.\n",
        "  - Hyperparameter tuning: New `training_args` with 5 epochs, batch size 4, learning rate 5e-5, and increased logging steps.\n",
        "\n",
        "- **Trainer Initialization**:\n",
        "  - `trainer` is initialized with `model`, `training_args`, and `train_dataset`.\n",
        "\n",
        "- **Training**:\n",
        "  - `trainer.train()` initiates the training process.\n",
        "\n",
        "- **Evaluation**:\n",
        "  - `results` from `trainer.evaluate(eval_dataset=dataset)` provides evaluation metrics after training.\n",
        "\n"
      ],
      "metadata": {
        "id": "vT_XipqEY4d1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class sampleDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_len):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx]\n",
        "    label= self.labels[idx]\n",
        "    encoding = self.tokenizer(text, truncation= True, padding='max_length', max_length= self.max_len, return_tensors = 'pt')\n",
        "\n",
        "    return {\n",
        "\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label,dtype=torch.long)\n",
        "\n",
        "        }\n",
        "\n",
        "# prepare the dataset\n",
        "texts = df['review'].head(2000)\n",
        "labels= df['labels'].head(2000)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset = sampleDataset(texts, labels, tokenizer, max_len=32)\n",
        "\n",
        "# create dataloader\n",
        "dataloader = DataLoader(dataset, batch_size=2)\n",
        "\n",
        "# load Model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)\n",
        "\n",
        "# # training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir = '.results',  # output directory\n",
        "#     num_train_epochs = 1, # number of training epochs\n",
        "#     per_device_train_batch_size=2, # batch size for training\n",
        "#     logging_dir ='./logs',\n",
        "\n",
        "# )\n",
        "\"\"\"The above parameter are working fine\n",
        "{'eval_loss': 0.5178326368331909, 'eval_runtime': 15.8858, 'eval_samples_per_second': 125.899, 'eval_steps_per_second': 15.737, 'epoch': 1.0}\n",
        "\"\"\"\n",
        "\n",
        "# Hyper Parameter Tuning\n",
        "# Define training arguments with different hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    learning_rate=5e-5,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\"\"\"We are trying to do hyper parameter tuning and see the results\n",
        "{'eval_loss': 0.02625814639031887, 'eval_runtime': 15.9143, 'eval_samples_per_second': 125.673, 'eval_steps_per_second': 15.709, 'epoch': 5.0}\n",
        "\n",
        "Lower Evaluation Loss: eval_loss 0.026 from epoch 5 is significantly lower than eval_loss 0.51 from epoch 1.\n",
        " This suggests that the model's performance improved substantially as training progressed. Therefore, eval_loss 0.026 is better than eval_loss 0.51.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# trainer\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args= training_args,\n",
        "    train_dataset = dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# evaluate the model\n",
        "results = trainer.evaluate(eval_dataset=dataset)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Am7exxCOeGf",
        "outputId": "cfcfe0aa-12c8-4099-fc78-a01526f4099b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 06:05, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.701300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.684800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.727700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.664700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.711800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.719600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.701000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.713400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.635100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.652000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.652000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.667800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.570500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.584800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.566700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.520100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.634300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.627800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.715900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.565600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.546500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.592000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.674900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.658900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.716100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.579600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.544300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.655800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.511300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.537700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.685100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.715100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.399300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.446500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.773200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.609400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.532900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.359300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.398300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.690300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.646900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.378100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.372100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.678500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.491700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.413900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.667600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.672500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.627000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.754200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.591900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.353100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.431400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.633100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.487800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.868300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.278200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>1.011900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.470400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.323000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.754600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.431700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.446700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.481100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.889200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.148600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.721400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.584700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.706100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.850900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.429200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.512400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.205700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.603100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.520300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.581600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.247400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.518100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.969000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.588300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.121600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.299300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.329400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.228200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.458600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.131500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.151600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.182000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.295000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.116000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.355000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.308700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.134000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.206100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.338700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.030300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.256400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.417400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.418300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.284100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.498800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.256100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.084800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.232200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>0.216600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>0.072000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.073200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>0.268000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.317700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.337100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>0.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.343800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.417300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>0.469000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.295200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>0.149600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.023100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>0.340200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>0.157200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>0.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.138500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>0.176900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>0.169900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>0.561800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>0.439500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.256000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>0.003500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>0.072100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.010400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>0.343900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.159600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>0.186900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1630</td>\n",
              "      <td>0.146800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>0.458400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>0.007100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1670</td>\n",
              "      <td>0.263800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>0.153100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1690</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1710</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1730</td>\n",
              "      <td>0.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>0.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1790</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.032100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1810</td>\n",
              "      <td>0.193700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1830</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>0.138100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>0.179300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1890</td>\n",
              "      <td>0.082700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1910</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1930</td>\n",
              "      <td>0.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>0.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.224400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1970</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>0.195500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990</td>\n",
              "      <td>0.505600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2010</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2020</td>\n",
              "      <td>0.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2030</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2040</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2060</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2070</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2080</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2090</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.195200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2110</td>\n",
              "      <td>0.184100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2120</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2130</td>\n",
              "      <td>0.189000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2140</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2160</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2170</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2180</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2190</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.060200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2210</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2220</td>\n",
              "      <td>0.191900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2230</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2240</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2260</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2270</td>\n",
              "      <td>0.185600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2280</td>\n",
              "      <td>0.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2290</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2320</td>\n",
              "      <td>0.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2330</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2340</td>\n",
              "      <td>0.037800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2370</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2380</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2390</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2410</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2420</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2430</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2440</td>\n",
              "      <td>0.349800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.196000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2460</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2470</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2480</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2490</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:15]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.02625814639031887, 'eval_runtime': 15.9143, 'eval_samples_per_second': 125.673, 'eval_steps_per_second': 15.709, 'epoch': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "KpCCtU0RfjF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./fine_tuned_bert_model\"\n",
        "trainer.save_model(model_path)"
      ],
      "metadata": {
        "id": "vgm0d9lqQ2OQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laod the Model"
      ],
      "metadata": {
        "id": "-oCYfXzefv_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  transformers import BertForSequenceClassification\n",
        "\n",
        "model= BertForSequenceClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "PMkMi7TwftRD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Model on New Data"
      ],
      "metadata": {
        "id": "4HwoF1CGf_pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of testing on New data\n",
        "new_texts = test_reviews\n"
      ],
      "metadata": {
        "id": "k8QAGe3af-PY"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the new data"
      ],
      "metadata": {
        "id": "1YEEEEM6gX6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(new_texts, truncation= True, padding='max_length', max_length=32, return_tensors = 'pt')\n"
      ],
      "metadata": {
        "id": "TtPhkZvvgXdW"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Inputs to Pytorch"
      ],
      "metadata": {
        "id": "OC3e44fMgzIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert inputs to pytorch tensors\n",
        "input_ids = inputs['input_ids']\n",
        "attention_mask = inputs['attention_mask']"
      ],
      "metadata": {
        "id": "I2xJiub8gypJ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Predictions"
      ],
      "metadata": {
        "id": "KEHLW8mkizBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs= model(input_ids, attention_mask=attention_mask)"
      ],
      "metadata": {
        "id": "Rl8Yn25wg_XY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract Predicted labels"
      ],
      "metadata": {
        "id": "sXusKUiCjGdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = torch.argmax(outputs.logits, dim=1)"
      ],
      "metadata": {
        "id": "sBBN9IDZjEnd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQZEpVVgQjie",
        "outputId": "c9fbc25e-4073-4218-abd7-62761cb6f9e7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
              "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
              "        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# display Predictions\n"
      ],
      "metadata": {
        "id": "BPwvt5MJjQz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text , label in zip(new_texts[:10], predicted_labels[:10]):\n",
        "  if label == 0:\n",
        "    label = \"NEGATIVE\"\n",
        "    print(f\"Text:{text[:100]} ---> Predicted Label :{label}\")\n",
        "  else:\n",
        "    label = \"POSITIVE\"\n",
        "    print(f\"Text:{text[:100]} ---> Predicted Label :{label}\")\n"
      ],
      "metadata": {
        "id": "k65EWRiqjPn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a04f95-1ac2-48b4-da5c-e3c18cd82444"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The ---> Predicted Label :POSITIVE\n",
            "Text:A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-B ---> Predicted Label :POSITIVE\n",
            "Text:I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air con ---> Predicted Label :POSITIVE\n",
            "Text:Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his par ---> Predicted Label :NEGATIVE\n",
            "Text:Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers  ---> Predicted Label :POSITIVE\n",
            "Text:Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble ca ---> Predicted Label :POSITIVE\n",
            "Text:I sure would like to see a resurrection of a up dated Seahunt series with the tech they have today i ---> Predicted Label :POSITIVE\n",
            "Text:This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8  ---> Predicted Label :POSITIVE\n",
            "Text:Encouraged by the positive comments about this film on here I was looking forward to watching this f ---> Predicted Label :NEGATIVE\n",
            "Text:If you like original gut wrenching laughter you will like this movie. If you are young or old then y ---> Predicted Label :POSITIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels"
      ],
      "metadata": {
        "id": "AbyjTPXMjjhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c71d70a1-3b4a-49c4-ec45-9b33845a0cfc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Metrics"
      ],
      "metadata": {
        "id": "iXRSe6pxFvdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# true labels for your new_texts\n",
        "true_labels = test_labels\n",
        "\n",
        "# classification report\n",
        "print(classification_report(true_labels , predicted_labels))\n",
        "\n",
        "# confusion matrix\n",
        "print(confusion_matrix(true_labels , predicted_labels))"
      ],
      "metadata": {
        "id": "ki4eWcO5j00C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1562b0fe-e6fc-4b8f-faab-0c8661bb781b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87        58\n",
            "           1       0.81      0.83      0.82        42\n",
            "\n",
            "    accuracy                           0.85       100\n",
            "   macro avg       0.85      0.85      0.85       100\n",
            "weighted avg       0.85      0.85      0.85       100\n",
            "\n",
            "[[50  8]\n",
            " [ 7 35]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are my inputs based on the classification results:\n",
        "\n",
        "- **Accuracy**: The model achieves an accuracy of 85%, indicating that it correctly predicts the class for 85 out of every 100 instances.\n",
        "\n",
        "- **Precision**:\n",
        "  - Class 0 (negative class): Precision of 88% means that when the model predicts an instance as class 0, it is correct 88% of the time.\n",
        "  - Class 1 (positive class): Precision of 81% indicates that when the model predicts an instance as class 1, it is correct 81% of the time.\n",
        "\n",
        "- **Recall**:\n",
        "  - Class 0: Recall of 86% suggests that the model correctly identifies 86% of all actual class 0 instances.\n",
        "  - Class 1: Recall of 83% indicates that the model correctly identifies 83% of all actual class 1 instances.\n",
        "\n",
        "- **F1-score**:\n",
        "  - Class 0: F1-score of 87% balances precision and recall for class 0.\n",
        "  - Class 1: F1-score of 82% balances precision and recall for class 1.\n",
        "\n",
        "- **Support**:\n",
        "  - Class 0: Represents 58 instances in the dataset.\n",
        "  - Class 1: Represents 42 instances in the dataset.\n",
        "\n",
        "- **Macro Average**:\n",
        "  - Precision, recall, and F1-score are all 85%, computed by averaging their respective values across classes, treating each class equally.\n",
        "\n",
        "- **Weighted Average**:\n",
        "  - Precision, recall, and F1-score are all 85%, computed by averaging their respective values across classes, weighted by the number of instances for each class.\n",
        "\n",
        "Overall, the model performs well with balanced precision and recall scores, indicating robust performance across both classes in the dataset. Adjustments and improvements can be made based on specific goals and requirements for further optimization."
      ],
      "metadata": {
        "id": "AXZ6H3E3Z2v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe for actual and predicted values\n",
        "pred_df = pd.DataFrame({\n",
        "    'Text': new_texts,\n",
        "    'Actual Label': test_labels,\n",
        "    'Predicted Label': predicted_labels\n",
        "})\n",
        "\n",
        "# Display the DataFrame\n",
        "pred_df.head() # Print the first few rows to verify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Bi0clRIoQ1k-",
        "outputId": "f3236a59-4d69-4d24-9a53-61cc3c8eee1b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  Actual Label  \\\n",
              "0  One of the other reviewers has mentioned that ...             1   \n",
              "1  A wonderful little production. <br /><br />The...             1   \n",
              "2  I thought this was a wonderful way to spend ti...             1   \n",
              "3  Basically there's a family where a little boy ...             0   \n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...             1   \n",
              "\n",
              "   Predicted Label  \n",
              "0                1  \n",
              "1                1  \n",
              "2                1  \n",
              "3                0  \n",
              "4                1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4209ef9e-dff9-41c1-958a-eccfc2de4c61\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Actual Label</th>\n",
              "      <th>Predicted Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4209ef9e-dff9-41c1-958a-eccfc2de4c61')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4209ef9e-dff9-41c1-958a-eccfc2de4c61 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4209ef9e-dff9-41c1-958a-eccfc2de4c61');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-34420ff3-48a7-4ba1-9169-f7300692a13b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-34420ff3-48a7-4ba1-9169-f7300692a13b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-34420ff3-48a7-4ba1-9169-f7300692a13b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pred_df",
              "summary": "{\n  \"name\": \"pred_df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"\\\"Fate\\\" leads Walter Sparrow to come in possession of a mysterious novel that has eerie similarities and connections to his life, all based around the number 23. As the story unfolds in real life and fiction, Sparrow must figure out his connection to the book and how the story will eventually end.<br /><br />The Number 23 offers an intriguing premise that is undone by a weak execution. The film just failed on many different levels which is pretty disappointing because it held so much potential. The screenplay was probably the worst part about it. It was filled with silly sequences and laughable dialog that just killed the mood of the movie. It seemed like the screenwriter had a good idea, he just didn't know how to develop it to stretch over a ninety minute running time. The second half of the film was running low on ideas, the twist was pretty obvious and the ending was awful.<br /><br />Joel Schumacher is responsible for one of the worst movies ever and he did redeem himself a little with Phone Booth and a few other films but The Number 23 reminds me that he's still capable of making a stinker. He has the movie drenched in style but he just can't get a good focus. He moves the film at a clunky and slow pace. He switches from reality to what's actually happening in the book which quickly got annoying. The actual book in the film that's titled \\\"The Number 23\\\" is an awful detective story and the audience gets stuck listening to Carrey narrate it which just bored me to tears. When Carrey is finally done with book, we get stuck watching him run around trying to solve the mystery. At this point, the audience has lost interest and there is no real tension. We impatiently wait for the movie to reach it's horrible ending and unconvincing explanation before celebrating that film has finally finished.<br /><br />The acting was mostly average and pretty forgettable. Jim Carrey was clearly just sleepwalking through his performance and he didn't even seem to be trying. He was either completely over the top in some scenes or just very wooden. His narration was a complete bore to listen to and he put no life inside his character. Virginia Madsen did the best she could with a limited role but she needs to pick better scripts. Logan Lerman was pretty bland as was Danny Huston. Overall, The Number 23 was an awful thriller that offered more laughs than suspense or thrills. Rating 3/10\",\n          \"I cannot believe I enjoyed this as much as I did. The anthology stories were better than par, but the linking story and its surprise ending hooked me. Alot of familiar faces will keep you asking yourself \\\"where I have I seen them before?\\\" Forget the running time listed on New Line's tape, this ain't no 103 minutes, according to my VCR timer and IMDB. Space Maggot douses the campfire in his own special way and hikes this an 8.\",\n          \"Caddyshack Two is a good movie by itself but compared to the original it cant stack up. Robert Stack is a horrible replacement for Ted Knight and Jackie Mason, while funny just cant compete with Rodney Dangerfield. Ty Webb is funny, being the only character from the original. Most of the other characters in the movie lack the punch of the original (Henry Wilcoxon for example) except for the hystericly funny lawyer Peter Blunt, being played by Randy Quaid. Every line he says reminds me of the originals humor, especially the scene at his office (I don't go in for law suits or motions. I find out where you live and come to your house and beat down your door with a f***ing baseball bat, make a bonfire with the chippindale,maybe roast that golden retriever (arff arff arff) then eat it. And then I'm comin' upstairs junior, and I'm grabbing you by your brooks brothers pjs, and cramming your brand new BMW up your tight a**! Do we have an understanding?). Offsetting his small role however, is Dan Acroyd, who is obviously no replacement for Bill Murray. His voice is beyond irritating and everything he does isnt even funny, its just stupid. Overall Caddyshack II is a good movie, but in comparison to the awesome original it just cant cut it.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predicted Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Summary\n",
        "\n",
        "This project utilized BERT, a transformer-based model, to conduct sentiment analysis on the IMDB dataset. By fine-tuning BERT for sequence classification, the model accurately predicted sentiment labels (positive or negative) from movie reviews. Key findings include:\n",
        "- Achieved high accuracy, precision, recall, and F1-score metrics.\n",
        "- Demonstrated BERT's effectiveness in understanding and classifying sentiment in natural language.\n",
        "- Highlighted practical applications in sentiment analysis across various domains.\n",
        "  \n",
        "Overall, the project showcases the power of advanced NLP techniques in extracting meaningful insights from textual data.\n"
      ],
      "metadata": {
        "id": "R5z4WCgbeCpM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-K6pBt7-eDOJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}